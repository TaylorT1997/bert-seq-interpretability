[config]
experiment_name = trial3
dataset = fce_v1
model_name = roberta-base
max_seq_length = 128
per_device_train_batch_size = 16
per_device_eval_batch_size = 64
num_train_epochs = 20
warmup_ratio = 0.1
learning_rate = 2e-5
weight_decay = 0.1
seed = 4
adam_epsilon = 1e-8
test_label_dummy = test
make_all_labels_equal_max = True
is_seq_class = True
lowercase = True
gradient_accumulation_steps = 1
save_steps = 500
logging_steps = 500
output_dir = models/{experiment_name}/{model_name}/{dataset_name}/{datetime}/
do_mask = True
mask_prob = 0.15