[config]
experiment_name = final_test_layer={}_head={}
dataset = conll10
dataset_split = test
method = model_attention
model_name = roberta-base
results_input_dir = token_preds/{method}/{experiment_name}/{model_name}/{dataset_name}/
results_input_filename = token_scores.tsv
importance_threshold = 0.32
top_count = 0
preds_output_filename = token_label_pred.tsv
eval_results_filename = token_results.txt 
max_seq_length = 128
per_device_eval_batch_size = 64
seed = 15
test_label_dummy = test
lowercase = True
do_mask_words = False
mask_prob = 0.0
hid_to_attn_dropout = 0.10
attention_evidence_size = 100
final_hidden_layer_size = 300
initializer_name = glorot
attention_activation = soft
soft_attention = False
soft_attention_gamma = 0.01
