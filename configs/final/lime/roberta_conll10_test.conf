[config]
experiment_name = final_test
dataset = conll10
dataset_split = test
model_path = models/final_seq_class/roberta-base/conll10/20200915T164835/final_model/
model_name = roberta-base
max_seq_length = 128
per_device_eval_batch_size = 64
seed = 15
test_label_dummy = test
make_all_labels_equal_max = True
is_seq_class = True
lowercase = True
method = lime
output_file = token_preds/{method}/{experiment_name}/{model_name}/{dataset_name}/{datetime}/lime_importances.tsv
lime_num_samples = 4096
do_mask_words = False
mask_prob = 0.0
hid_to_attn_dropout = 0.10
attention_evidence_size = 100
final_hidden_layer_size = 300
initializer_name = glorot
attention_activation = soft
soft_attention = False
soft_attention_gamma = 0.01
